{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e514e5ea",
   "metadata": {},
   "source": [
    "# Maximally Advanced Pain Intensity Prediction Pipeline\n",
    "This notebook implements a state-of-the-art, strategy-driven pipeline for pain intensity prediction from biosignals (EMG, SCL, ECG). It includes robust denoising, comprehensive feature engineering (including HRV, SCR, Poincaré, cross-channel, and relative features), per-segment normalization, class balancing, stacking ensemble, model-based feature selection, domain adaptation, interpretability (SHAP), and robust evaluation (learning/ROC curves, LOSO CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and Setup ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy.signal import butter, filtfilt, iirnotch, resample, find_peaks\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional: Domain adaptation (CORAL)\n",
    "def coral(Xs, Xt):\n",
    "    # CORAL: Aligns source and target covariances\n",
    "    cov_src = np.cov(Xs, rowvar=False) + np.eye(Xs.shape[1])\n",
    "    cov_tar = np.cov(Xt, rowvar=False) + np.eye(Xt.shape[1])\n",
    "    U_src, S_src, _ = np.linalg.svd(cov_src)\n",
    "    U_tar, S_tar, _ = np.linalg.svd(cov_tar)\n",
    "    A_src = U_src @ np.diag(1.0/np.sqrt(S_src)) @ U_src.T\n",
    "    A_tar = U_tar @ np.diag(np.sqrt(S_tar)) @ U_tar.T\n",
    "    return (Xs - Xs.mean(0)) @ A_src @ A_tar + Xt.mean(0)\n",
    "\n",
    "# --- Signal Processing Utilities ---\n",
    "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low, high = lowcut/nyq, highcut/nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "def notch_filter(data, fs, freq=50.0, Q=30):\n",
    "    nyq = 0.5 * fs\n",
    "    w0 = freq / nyq\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "def wavelet_denoise(sig, wavelet='db4', level=2):\n",
    "    coeffs = pywt.wavedec(sig, wavelet, level=level)\n",
    "    sigma = np.median(np.abs(coeffs[-level])) / 0.6745\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(sig)))\n",
    "    denoised_coeffs = [coeffs[0]] + [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]\n",
    "    return pywt.waverec(denoised_coeffs, wavelet)[:len(sig)]\n",
    "\n",
    "# Optional: ICA for EMG artifact rejection (if sklearn.decomposition.FastICA is available)\n",
    "def try_ica(sig, n_components=1):\n",
    "    try:\n",
    "        from sklearn.decomposition import FastICA\n",
    "        ica = FastICA(n_components=n_components, random_state=42, max_iter=200)\n",
    "        sig_ica = ica.fit_transform(sig.reshape(-1,1)).flatten()\n",
    "        return sig_ica\n",
    "    except Exception:\n",
    "        return sig\n",
    "\n",
    "def zscore_log_transform(x):\n",
    "    x = np.array(x, dtype=float)\n",
    "    x = np.log1p(np.abs(x)) * np.sign(x)\n",
    "    mu, sigma = np.mean(x), np.std(x)\n",
    "    return (x - mu) / sigma if sigma > 0 else np.zeros_like(x)\n",
    "\n",
    "# --- HRV & Poincaré Features for ECG ---\n",
    "def hrv_features(sig, fs):\n",
    "    sig = np.nan_to_num(sig)\n",
    "    peaks, _ = find_peaks(sig, distance=fs*0.5)\n",
    "    rr = np.diff(peaks)/fs if len(peaks) > 1 else np.array([0])\n",
    "    hr = 60/rr.mean() if rr.mean() > 0 else 0\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(rr)))) if len(rr) > 1 else 0\n",
    "    sdnn = rr.std() if len(rr) > 1 else 0\n",
    "    # Frequency domain\n",
    "    if len(rr) > 2:\n",
    "        rr_interp = np.interp(np.arange(0, len(rr)), np.arange(0, len(rr)), rr)\n",
    "        freqs = np.fft.rfftfreq(len(rr_interp), 1/fs)\n",
    "        psd = np.abs(np.fft.rfft(rr_interp))**2\n",
    "        lf = psd[(freqs >= 0.04) & (freqs < 0.15)].sum()\n",
    "        hf = psd[(freqs >= 0.15) & (freqs < 0.4)].sum()\n",
    "        lf_hf = np.log1p(lf/hf) if hf > 0 else 0\n",
    "    else:\n",
    "        lf = hf = lf_hf = 0\n",
    "    # Poincaré\n",
    "    sd1 = np.sqrt(np.std(rr[:-1] - rr[1:])**2/2) if len(rr) > 2 else 0\n",
    "    sd2 = np.sqrt(2*np.std(rr)**2 - 0.5*np.std(rr[:-1] - rr[1:])**2) if len(rr) > 2 else 0\n",
    "    return [hr, rmssd, sdnn, lf, hf, lf_hf, sd1, sd2]\n",
    "\n",
    "# --- SCR Features for SCL ---\n",
    "def scr_features(sig, fs):\n",
    "    # Find SCRs as rapid increases in SCL\n",
    "    diff = np.diff(sig)\n",
    "    scr_peaks, _ = find_peaks(diff, height=np.std(diff), distance=fs*0.5)\n",
    "    n_scr = len(scr_peaks)\n",
    "    scr_amp = np.mean(diff[scr_peaks]) if n_scr > 0 else 0\n",
    "    auc = np.trapz(sig)\n",
    "    slope = (sig[-1] - sig[0]) / len(sig)\n",
    "    return [n_scr, scr_amp, auc, slope]\n",
    "\n",
    "# --- Feature Extraction (Comprehensive) ---\n",
    "def extract_all_features(data, fs, ch_names):\n",
    "    feats = []\n",
    "    for idx, kind in enumerate(['EMG','EMG','EMG','SCL','ECG']):\n",
    "        sig = data[:, idx]\n",
    "        # Filtering and denoising\n",
    "        if kind == 'EMG':\n",
    "            sig = bandpass_filter(sig, fs, 20, 450)\n",
    "            sig = notch_filter(sig, fs, 50)\n",
    "            sig = try_ica(sig)\n",
    "        elif kind == 'ECG':\n",
    "            sig = bandpass_filter(sig, fs, 0.5, 50)\n",
    "            sig = notch_filter(sig, fs, 50)\n",
    "        elif kind == 'SCL':\n",
    "            sig = bandpass_filter(sig, fs, 0.05, 5)\n",
    "        sig = resample(sig, int(len(sig)//(fs/250))) if fs > 250 else sig\n",
    "        sig = wavelet_denoise(sig)\n",
    "        # Time-domain\n",
    "        rms = np.sqrt(np.mean(sig**2))\n",
    "        log_rms = np.log1p(rms)\n",
    "        std = np.std(sig)\n",
    "        wl = np.sum(np.abs(np.diff(sig)))\n",
    "        zcr = ((sig[:-1]*sig[1:])<0).sum() / len(sig)\n",
    "        feats.extend([rms, log_rms, std, wl, zcr, skew(sig), kurtosis(sig)])\n",
    "        # Frequency-domain\n",
    "        freqs = np.fft.rfftfreq(len(sig), 1/250)\n",
    "        psd = np.abs(np.fft.rfft(sig))**2\n",
    "        feats.append(np.sum(psd[(freqs>=0.5)&(freqs<4)]))\n",
    "        feats.append(np.sum(psd[(freqs>=4)&(freqs<8)]))\n",
    "        feats.append(np.sum(psd[(freqs>=8)&(freqs<13)]))\n",
    "        feats.append(np.sum(psd[(freqs>=13)&(freqs<30)]))\n",
    "        feats.append(np.sum(psd[(freqs>=30)&(freqs<45)]))\n",
    "        # Wavelet energy/entropy\n",
    "        coeffs = pywt.wavedec(sig, 'db4', level=3)\n",
    "        energies = [np.sum(np.square(c)) for c in coeffs]\n",
    "        total_energy = np.sum(energies)\n",
    "        ent = -np.sum([(e/total_energy)*np.log(e/total_energy+1e-8) if e>0 else 0 for e in energies])\n",
    "        feats.extend(energies + [ent])\n",
    "        # ECG HRV/Poincaré\n",
    "        if kind == 'ECG':\n",
    "            feats.extend(hrv_features(sig, 250))\n",
    "        # SCL SCR\n",
    "        if kind == 'SCL':\n",
    "            feats.extend(scr_features(sig, 250))\n",
    "    # Cross-channel features (correlations, ratios)\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(i+1, data.shape[1]):\n",
    "            feats.append(np.corrcoef(data[:,i], data[:,j])[0,1])\n",
    "            feats.append(np.mean(data[:,i]) / (np.mean(data[:,j])+1e-8))\n",
    "    # Delta features (change from start to end)\n",
    "    for idx in range(data.shape[1]):\n",
    "        sig = data[:, idx]\n",
    "        feats.append(sig[-1] - sig[0])\n",
    "    return np.nan_to_num(feats)\n",
    "\n",
    "# --- Data Loader ---\n",
    "def collect_segments_all(root_dir):\n",
    "    segments, labels, subjects, pain_map = [], [], [], {}\n",
    "    for pain_label in sorted(os.listdir(root_dir)):\n",
    "        pain_path = os.path.join(root_dir, pain_label)\n",
    "        if not os.path.isdir(pain_path): continue\n",
    "        for subj in sorted(os.listdir(pain_path)):\n",
    "            subj_path = os.path.join(pain_path, subj)\n",
    "            if not os.path.isdir(subj_path): continue\n",
    "            for segfile in sorted(os.listdir(subj_path)):\n",
    "                if not segfile.endswith('.mat'): continue\n",
    "                matf = os.path.join(subj_path, segfile)\n",
    "                try:\n",
    "                    mat = scipy.io.loadmat(matf)\n",
    "                    data = mat['data']\n",
    "                    fs = int(mat['fs'].ravel()[0])\n",
    "                    ch_names = [str(s[0]) if isinstance(s, np.ndarray) else str(s) for s in mat['labels'].ravel()]\n",
    "                    feats = extract_all_features(data, fs, ch_names)\n",
    "                    segments.append(feats)\n",
    "                    labels.append(pain_label)\n",
    "                    subjects.append(subj)\n",
    "                    pain_map[pain_label] = pain_map.get(pain_label, len(pain_map))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {matf}: {e}\")\n",
    "    return np.array(segments), np.array(labels), np.array(subjects), pain_map\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "root_dir = r\"D:\\bio_s\"  # Change if needed\n",
    "print(\"Loading and extracting all advanced features...\")\n",
    "X, y, subjects, pain_map = collect_segments_all(root_dir)\n",
    "if len(X) == 0:\n",
    "    raise RuntimeError(\"No valid segments loaded. Please check your data and preprocessing functions.\")\n",
    "print(f\"Total segments: {len(X)}, Features per segment: {X.shape[1]}\")\n",
    "label_names = sorted(pain_map, key=lambda k: pain_map[k])\n",
    "y_enc = np.array([pain_map[lab] for lab in y])\n",
    "\n",
    "# --- Per-segment normalization and log transform ---\n",
    "X = np.apply_along_axis(zscore_log_transform, 1, X)\n",
    "\n",
    "# --- Class Balancing ---\n",
    "sm = SMOTE(random_state=42)\n",
    "X_bal, y_bal = sm.fit_resample(X, y_enc)\n",
    "\n",
    "# --- Model-based Feature Selection ---\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')\n",
    "sel.fit(X_bal, y_bal)\n",
    "X_sel = sel.transform(X_bal)\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y_bal, test_size=0.3, stratify=y_bal, random_state=42)\n",
    "\n",
    "# --- Domain Adaptation (CORAL) ---\n",
    "try:\n",
    "    X_train = coral(X_train, X_test)\n",
    "except Exception as e:\n",
    "    print(f\"CORAL domain adaptation skipped: {e}\")\n",
    "\n",
    "# --- Stacking Ensemble (XGB, LGBM, SVM, meta-learner) ---\n",
    "base_learners = [\n",
    "    ('xgb', XGBClassifier(n_estimators=200, max_depth=6, eval_metric='mlogloss', use_label_encoder=False, random_state=42)),\n",
    "    ('lgbm', LGBMClassifier(n_estimators=200, max_depth=6, class_weight='balanced', random_state=42)),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)),\n",
    "]\n",
    "meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "stack = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=3, n_jobs=-1, passthrough=True)\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__max_depth': [4, 6],\n",
    "    'lgbm__n_estimators': [100, 200],\n",
    "    'lgbm__max_depth': [4, 6],\n",
    "    'svm__C': [1, 10],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'final_estimator__max_depth': [4, 8],\n",
    "}\n",
    "grid = GridSearchCV(stack, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluation ---\n",
    "y_pred = grid.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# --- Learning Curve ---\n",
    "train_sizes, train_scores, test_scores = learning_curve(grid.best_estimator_, X_train, y_train, cv=3, n_jobs=-1)\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Train')\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Validation')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve (One-vs-Rest) ---\n",
    "from sklearn.preprocessing import label_binarize\n",
    "n_classes = len(np.unique(y_test))\n",
    "y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "y_score = grid.predict_proba(X_test)\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {label_names[i]} (AUC={roc_auc[i]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (One-vs-Rest)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP Feature Importance (XGB, LGBM) ---\n",
    "for name in ['xgb', 'lgbm']:\n",
    "    try:\n",
    "        model = grid.best_estimator_.named_estimators_[name]\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test, feature_names=[f'f{i}' for i in range(X_test.shape[1])], show=False)\n",
    "        plt.title(f'SHAP Feature Importance ({name.upper()})')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP plot error for {name}: {e}\")\n",
    "\n",
    "# --- LOSO CV for Robustness ---\n",
    "accs = []\n",
    "all_true = []\n",
    "all_pred = []\n",
    "for subj in np.unique(subjects):\n",
    "    test_idx = (subjects == subj)\n",
    "    train_idx = ~test_idx\n",
    "    if np.sum(test_idx) == 0 or np.sum(train_idx) == 0:\n",
    "        continue\n",
    "    Xtr, Xte = X[train_idx], X[test_idx]\n",
    "    ytr, yte = y_enc[train_idx], y_enc[test_idx]\n",
    "    Xtr_bal, ytr_bal = sm.fit_resample(Xtr, ytr)\n",
    "    Xtr_sel = sel.transform(Xtr_bal)\n",
    "    Xte_sel = sel.transform(Xte)\n",
    "    try:\n",
    "        Xtr_sel = coral(Xtr_sel, Xte_sel)\n",
    "    except Exception:\n",
    "        pass\n",
    "    model = grid.best_estimator_\n",
    "    model.fit(Xtr_sel, ytr_bal)\n",
    "    ypred = model.predict(Xte_sel)\n",
    "    accs.append(accuracy_score(yte, ypred))\n",
    "    all_true.extend(yte)\n",
    "    all_pred.extend(ypred)\n",
    "print(f\"\\nLOSO Mean Accuracy: {np.mean(accs):.3f}\")\n",
    "print(\"\\nLOSO Classification Report:\")\n",
    "print(classification_report(all_true, all_pred, target_names=label_names))\n",
    "print(\"LOSO Confusion Matrix:\")\n",
    "print(confusion_matrix(all_true, all_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a3750",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "- This pipeline uses robust denoising, comprehensive feature engineering (including HRV, SCR, Poincaré, cross-channel, and relative features), per-segment normalization, class balancing, model-based feature selection, domain adaptation, stacking ensemble, and interpretability (SHAP).\n",
    "- It provides learning/ROC curves and robust LOSO CV evaluation for generalization.\n",
    "- Tune the feature set or model parameters further if needed to maximize accuracy and recall."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
