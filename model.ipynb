{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7309604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn feature selection to integrate into pipeline.\n",
    "    Supports 'kbest', 'rfe', or 'none'.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='kbest', k=20, estimator=None, score_func='f_classif'):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.estimator = estimator\n",
    "        self.score_func = score_func\n",
    "        self.selector_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'kbest':\n",
    "            func = f_classif if self.score_func == 'f_classif' else mutual_info_classif\n",
    "            self.selector_ = SelectKBest(score_func=func, k=self.k)\n",
    "        elif self.method == 'rfe':\n",
    "            base_est = self.estimator if self.estimator is not None else LogisticRegression(max_iter=1000)\n",
    "            self.selector_ = RFE(estimator=base_est, n_features_to_select=self.k)\n",
    "        elif self.method == 'none':\n",
    "            self.selector_ = None\n",
    "            return self\n",
    "        else:\n",
    "            raise ValueError(\"Unknown feature selection method.\")\n",
    "        self.selector_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.selector_ is None:\n",
    "            return X\n",
    "        return self.selector_.transform(X)\n",
    "\n",
    "    def get_support(self):\n",
    "        if self.selector_ is None:\n",
    "            return np.arange(X.shape[1])\n",
    "        return self.selector_.get_support()\n",
    "\n",
    "def build_model_pipeline(\n",
    "    feature_selection_method='kbest',\n",
    "    k_features=20,\n",
    "    selector_score_func='f_classif',\n",
    "    classifier='logreg',\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a ready-to-use sklearn Pipeline with feature selection and classifier.\n",
    "    \"\"\"\n",
    "    if classifier == 'logreg':\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    elif classifier == 'rf':\n",
    "        clf = RandomForestClassifier(random_state=random_state)\n",
    "    elif classifier == 'svm':\n",
    "        clf = SVC(probability=True, random_state=random_state)\n",
    "    elif classifier == 'lasso':\n",
    "        clf = LassoCV(cv=5, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown classifier\")\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Always normalize before selection and modeling\n",
    "        ('feature_selection', FeatureSelector(\n",
    "            method=feature_selection_method,\n",
    "            k=k_features,\n",
    "            estimator=clf if feature_selection_method == 'rfe' else None,\n",
    "            score_func=selector_score_func)),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def run_grid_search(\n",
    "    X, y,\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv_folds=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a GridSearchCV with the provided pipeline and param grid.\n",
    "    Returns the fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=cv, n_jobs=n_jobs, scoring=scoring, refit=True)\n",
    "    grid.fit(X, y)\n",
    "    return grid\n",
    "\n",
    "# Example usage:\n",
    "# pipeline = build_model_pipeline(\n",
    "#     feature_selection_method='kbest',  # or 'rfe', 'none'\n",
    "#     k_features=30,\n",
    "#     selector_score_func='f_classif',  # or 'mutual_info_classif'\n",
    "#     classifier='rf'  # 'logreg', 'rf', 'svm', 'lasso'\n",
    "# )\n",
    "#\n",
    "# param_grid = {\n",
    "#     'feature_selection__k': [10, 20, 30, 50],\n",
    "#     'classifier__n_estimators': [100, 200]  # If using 'rf'\n",
    "# }\n",
    "#\n",
    "# best_model = run_grid_search(X, y, pipeline, param_grid)\n",
    "# y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa90e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_feature_importance(model, feature_names=None):\n",
    "    \"\"\"\n",
    "    Extracts feature importance or coefficients from a fitted scikit-learn pipeline.\n",
    "\n",
    "    Args:\n",
    "        model: Fitted scikit-learn Pipeline (with feature selection and classifier).\n",
    "        feature_names: List of original feature names (optional). If not provided, uses generic names.\n",
    "\n",
    "    Returns:\n",
    "        importance_df: pandas DataFrame with feature names and importance values, sorted descending.\n",
    "    \"\"\"\n",
    "    # Get support mask after feature selection\n",
    "    selector = model.named_steps['feature_selection']\n",
    "    X_support = None\n",
    "    if hasattr(selector, 'get_support'):\n",
    "        X_support = selector.get_support()\n",
    "    if feature_names is not None and X_support is not None:\n",
    "        selected_names = np.array(feature_names)[X_support]\n",
    "    elif X_support is not None:\n",
    "        selected_names = [f'f{i}' for i in range(np.sum(X_support))]\n",
    "    else:\n",
    "        selected_names = feature_names if feature_names is not None else [f'f{i}' for i in range(model.named_steps['scaler'].n_features_in_)]\n",
    "\n",
    "    clf = model.named_steps['classifier']\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        importances = clf.feature_importances_\n",
    "    elif hasattr(clf, 'coef_'):\n",
    "        importances = np.abs(clf.coef_).ravel()\n",
    "    else:\n",
    "        raise AttributeError(\"Classifier does not provide feature importance or coefficients.\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": selected_names,\n",
    "        \"importance\": importances\n",
    "    }).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    return importance_df\n",
    "\n",
    "def batch_predict(model, X, batch_size=256, proba=False):\n",
    "    \"\"\"\n",
    "    Makes predictions or predicts probabilities in batches for large datasets.\n",
    "\n",
    "    Args:\n",
    "        model: Trained scikit-learn pipeline.\n",
    "        X: Feature matrix (np.ndarray or pd.DataFrame).\n",
    "        batch_size: Number of samples per batch.\n",
    "        proba: If True, returns class probabilities; else, class predictions.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predictions or probabilities for all samples.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    results = []\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        X_batch = X[start:end]\n",
    "        if proba and hasattr(model, \"predict_proba\"):\n",
    "            batch_result = model.predict_proba(X_batch)\n",
    "        else:\n",
    "            batch_result = model.predict(X_batch)\n",
    "        results.append(batch_result)\n",
    "    if proba:\n",
    "        return np.vstack(results)\n",
    "    return np.concatenate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0900bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "def compute_metrics(y_true, y_pred, labels=None, average='weighted'):\n",
    "    \"\"\"\n",
    "    Computes classification metrics and confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels (array-like).\n",
    "        y_pred: Predicted labels (array-like).\n",
    "        labels: Optional list of label names (for consistent confusion matrix display).\n",
    "        average: Averaging method for multi-class scores ('weighted', 'macro', etc.).\n",
    "\n",
    "    Returns:\n",
    "        metrics_dict: Dictionary containing accuracy, precision, recall, f1, classification report, and confusion matrix.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    cls_report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1,\n",
    "        \"classification_report\": cls_report,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n",
    "    return metrics_dict\n",
    "\n",
    "def print_metrics(metrics_dict, label_names=None):\n",
    "    \"\"\"\n",
    "    Pretty-prints the classification metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict: Output from compute_metrics.\n",
    "        label_names: Optional label names for confusion matrix rows/columns.\n",
    "    \"\"\"\n",
    "    print(\"Accuracy:\", metrics_dict[\"accuracy\"])\n",
    "    print(\"Precision:\", metrics_dict[\"precision\"])\n",
    "    print(\"Recall:\", metrics_dict[\"recall\"])\n",
    "    print(\"F1 Score:\", metrics_dict[\"f1_score\"])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = metrics_dict[\"classification_report\"]\n",
    "    if isinstance(report, dict):\n",
    "        print(pd.DataFrame(report).transpose())\n",
    "    else:\n",
    "        print(report)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = metrics_dict[\"confusion_matrix\"]\n",
    "    if label_names is not None:\n",
    "        df_cm = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "        print(df_cm)\n",
    "    else:\n",
    "        print(cm)\n",
    "\n",
    "def plot_confusion_matrix(metrics_dict, label_names=None, normalize=False, cmap='Blues', title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict: Output from compute_metrics.\n",
    "        label_names: List of class labels (for axis ticks).\n",
    "        normalize: If True, shows percentages instead of counts.\n",
    "        cmap: Colormap for plot.\n",
    "        title: Plot title.\n",
    "    \"\"\"\n",
    "    cm = metrics_dict[\"confusion_matrix\"]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    if label_names is not None:\n",
    "        ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "               xticklabels=label_names, yticklabels=label_names, title=title, ylabel='True label', xlabel='Predicted label')\n",
    "        plt.xticks(rotation=45)\n",
    "    else:\n",
    "        ax.set(title=title, ylabel='True label', xlabel='Predicted label')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# y_true = ... # ground truth labels\n",
    "# y_pred = ... # predicted labels\n",
    "# metrics = compute_metrics(y_true, y_pred, labels=[0,1,2])\n",
    "# print_metrics(metrics, label_names=['No pain', 'Mild pain', 'Severe pain'])\n",
    "# plot_confusion_matrix(metrics, label_names=['No pain', 'Mild pain', 'Severe pain'], normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
