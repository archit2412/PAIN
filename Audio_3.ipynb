{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "509ca26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created D:\\X-ITE Pain\\labels.csv with 3117 rows and columns: ['pain_level', 'subject', 'file_name', 'audio_path']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1: Create labels.csv for low_pain and medium_pain classes\n",
    "\n",
    "This script scans your audio dataset, assuming the following folder structure:\n",
    "    D:\\X-ITE Pain\\low_pain\\audio\\{subject}\\{file_name.wav}\n",
    "    D:\\X-ITE Pain\\medium_pain\\audio\\{subject}\\{file_name.wav}\n",
    "\n",
    "It creates a CSV file with columns:\n",
    "    pain_level, subject, file_name, audio_path\n",
    "\n",
    "Only \"low_pain\" and \"medium_pain\" classes are included.\n",
    "\n",
    "Instructions:\n",
    "- Update BASE_DIR if your data is elsewhere.\n",
    "- Run this script. It creates labels.csv in BASE_DIR.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = r'D:\\X-ITE Pain'  # Change this if your data is elsewhere\n",
    "PAIN_CLASSES = {\"low_pain\", \"medium_pain\"}  # Only use these classes\n",
    "\n",
    "rows = []\n",
    "for pain_level in os.listdir(BASE_DIR):\n",
    "    if pain_level not in PAIN_CLASSES:\n",
    "        continue\n",
    "    pain_path = os.path.join(BASE_DIR, pain_level, 'audio')\n",
    "    if not os.path.isdir(pain_path):\n",
    "        continue\n",
    "    for subject in os.listdir(pain_path):\n",
    "        subject_path = os.path.join(pain_path, subject)\n",
    "        if not os.path.isdir(subject_path):\n",
    "            continue\n",
    "        for f in os.listdir(subject_path):\n",
    "            if f.lower().endswith('.wav'):\n",
    "                rows.append({\n",
    "                    'pain_level': pain_level,\n",
    "                    'subject': subject,\n",
    "                    'file_name': f,\n",
    "                    'audio_path': os.path.join(subject_path, f)\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "labels_path = os.path.join(BASE_DIR, 'labels.csv')\n",
    "df.to_csv(labels_path, index=False)\n",
    "print(f\"Created {labels_path} with {len(df)} rows and columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92560e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced feature extraction complete. Saved to D:\\X-ITE Pain\\advanced_features.csv with shape (3117, 654)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2: Extract advanced audio features for low_pain and medium_pain samples\n",
    "\n",
    "- Loads labels.csv from Step 1.\n",
    "- For each audio file, extracts:\n",
    "    - MFCCs (mean, std, min, max, 13 coefficients)\n",
    "    - Delta MFCCs (mean, std, 13 coefficients)\n",
    "    - Chroma STFT (mean, std, 12 coefficients)\n",
    "    - Mel Spectrogram (mean, std, min, max, 128 bands)\n",
    "    - Spectral Contrast (mean, std, 7 bands)\n",
    "    - Tonnetz (mean, std, 6 dimensions)\n",
    "    - Spectral Centroid (mean, std)\n",
    "    - Spectral Bandwidth (mean, std)\n",
    "    - Spectral Rolloff (mean, std)\n",
    "    - Zero Crossing Rate (mean, std)\n",
    "    - RMS energy (mean, std)\n",
    "- Concatenates all features into one row per audio file.\n",
    "- Saves the features as advanced_features.csv in your BASE_DIR.\n",
    "\n",
    "Instructions:\n",
    "- Install: librosa, numpy, pandas\n",
    "- Update BASE_DIR if your data is elsewhere.\n",
    "- Run after Step 1.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "BASE_DIR = r'D:\\X-ITE Pain'  # Change if necessary\n",
    "labels_path = f\"{BASE_DIR}\\\\labels.csv\"\n",
    "FEATURES_PATH = f\"{BASE_DIR}\\\\advanced_features.csv\"\n",
    "SR = 44100\n",
    "N_MFCC = 13\n",
    "N_MELS = 128\n",
    "\n",
    "def extract_features(audio_path):\n",
    "    try:\n",
    "        y, _ = librosa.load(audio_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=N_MFCC)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "        features.extend(np.min(mfcc, axis=1))\n",
    "        features.extend(np.max(mfcc, axis=1))\n",
    "\n",
    "        # Delta MFCCs\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        features.extend(np.mean(mfcc_delta, axis=1))\n",
    "        features.extend(np.std(mfcc_delta, axis=1))\n",
    "\n",
    "        # Chroma STFT\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=SR)\n",
    "        features.extend(np.mean(chroma, axis=1))\n",
    "        features.extend(np.std(chroma, axis=1))\n",
    "\n",
    "        # Mel Spectrogram (log-mel)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=SR, n_mels=N_MELS)\n",
    "        log_mel = librosa.power_to_db(mel)\n",
    "        features.extend(np.mean(log_mel, axis=1))\n",
    "        features.extend(np.std(log_mel, axis=1))\n",
    "        features.extend(np.min(log_mel, axis=1))\n",
    "        features.extend(np.max(log_mel, axis=1))\n",
    "\n",
    "        # Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=SR)\n",
    "        features.extend(np.mean(contrast, axis=1))\n",
    "        features.extend(np.std(contrast, axis=1))\n",
    "\n",
    "        # Tonnetz\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        # Spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=SR)\n",
    "        features.append(np.mean(centroid))\n",
    "        features.append(np.std(centroid))\n",
    "\n",
    "        # Spectral bandwidth\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=SR)\n",
    "        features.append(np.mean(bandwidth))\n",
    "        features.append(np.std(bandwidth))\n",
    "\n",
    "        # Spectral rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=SR)\n",
    "        features.append(np.mean(rolloff))\n",
    "        features.append(np.std(rolloff))\n",
    "\n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features.append(np.mean(zcr))\n",
    "        features.append(np.std(zcr))\n",
    "\n",
    "        # RMS energy\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features.append(np.mean(rms))\n",
    "        features.append(np.std(rms))\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        # Return NaNs for all features if error\n",
    "        n_features = (N_MFCC*4 + N_MFCC*2 + 12*2 + N_MELS*4 + 7*2 + 6*2 + 2*5)\n",
    "        return [np.nan] * n_features\n",
    "\n",
    "# Build feature column names\n",
    "feature_names = (\n",
    "    [f\"mfcc_mean_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_std_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_min_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_max_{i+1}\" for i in range(N_MFCC)] +\n",
    "\n",
    "    [f\"mfcc_delta_mean_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_delta_std_{i+1}\" for i in range(N_MFCC)] +\n",
    "\n",
    "    [f\"chroma_mean_{i+1}\" for i in range(12)] +\n",
    "    [f\"chroma_std_{i+1}\" for i in range(12)] +\n",
    "\n",
    "    [f\"mel_mean_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_std_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_min_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_max_{i+1}\" for i in range(N_MELS)] +\n",
    "\n",
    "    [f\"contrast_mean_{i+1}\" for i in range(7)] +\n",
    "    [f\"contrast_std_{i+1}\" for i in range(7)] +\n",
    "\n",
    "    [f\"tonnetz_mean_{i+1}\" for i in range(6)] +\n",
    "    [f\"tonnetz_std_{i+1}\" for i in range(6)] +\n",
    "\n",
    "    [\"centroid_mean\", \"centroid_std\"] +\n",
    "    [\"bandwidth_mean\", \"bandwidth_std\"] +\n",
    "    [\"rolloff_mean\", \"rolloff_std\"] +\n",
    "    [\"zcr_mean\", \"zcr_std\"] +\n",
    "    [\"rms_mean\", \"rms_std\"]\n",
    ")\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "features = [extract_features(row['audio_path']) for _, row in df.iterrows()]\n",
    "feat_df = pd.DataFrame(features, columns=feature_names)\n",
    "result_df = pd.concat([df, feat_df], axis=1)\n",
    "result_df.to_csv(FEATURES_PATH, index=False)\n",
    "print(f\"Advanced feature extraction complete. Saved to {FEATURES_PATH} with shape {result_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9038145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2493 samples | Test set: 624 samples\n",
      "Saved: D:\\X-ITE Pain\\train_features.csv and D:\\X-ITE Pain\\test_features.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3: Stratified Train/Test Split for low_pain and medium_pain\n",
    "\n",
    "- Loads mfcc_features.csv produced in Step 2.\n",
    "- Performs a stratified split (so both classes are balanced in train and test).\n",
    "- Saves train_features.csv and test_features.csv in your BASE_DIR.\n",
    "\n",
    "Instructions:\n",
    "- Make sure BASE_DIR is set correctly.\n",
    "- Run this script after Step 2.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE_DIR = r'D:\\X-ITE Pain'  # Change if necessary\n",
    "FEATURES_PATH = f\"{BASE_DIR}\\\\advanced_features.csv\"\n",
    "TRAIN_PATH = f\"{BASE_DIR}\\\\train_features.csv\"\n",
    "TEST_PATH = f\"{BASE_DIR}\\\\test_features.csv\"\n",
    "\n",
    "# Load features\n",
    "df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "# Drop rows with missing features (if any)\n",
    "df = df.dropna()\n",
    "\n",
    "# Stratified split\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['pain_level'],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "train_df.to_csv(TRAIN_PATH, index=False)\n",
    "test_df.to_csv(TEST_PATH, index=False)\n",
    "print(f\"Train set: {len(train_df)} samples | Test set: {len(test_df)} samples\")\n",
    "print(f\"Saved: {TRAIN_PATH} and {TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9ee3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:42:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    low_pain       0.59      0.57      0.58       312\n",
      " medium_pain       0.59      0.61      0.60       312\n",
      "\n",
      "    accuracy                           0.59       624\n",
      "   macro avg       0.59      0.59      0.59       624\n",
      "weighted avg       0.59      0.59      0.59       624\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[178 134]\n",
      " [122 190]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4: Train and evaluate an XGBoost model for low_pain vs. medium_pain (with label encoding)\n",
    "\n",
    "- Loads train_features.csv and test_features.csv from Step 3.\n",
    "- Encodes string labels to integers (required for XGBoost).\n",
    "- Standardizes features using scikit-learn's StandardScaler.\n",
    "- Trains an XGBoost classifier.\n",
    "- Prints classification report and confusion matrix (with original string labels).\n",
    "- Saves the trained model and the label encoder.\n",
    "\n",
    "Instructions:\n",
    "- Make sure you have installed: xgboost, scikit-learn, joblib, pandas, numpy\n",
    "- Update BASE_DIR if your data is elsewhere.\n",
    "- Run this script after Step 3.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "BASE_DIR = r'D:\\X-ITE Pain'  # Change if necessary\n",
    "TRAIN_PATH = f\"{BASE_DIR}\\\\train_features.csv\"\n",
    "TEST_PATH = f\"{BASE_DIR}\\\\test_features.csv\"\n",
    "MODEL_PATH = f\"{BASE_DIR}\\\\xgb_audio_model.joblib\"\n",
    "ENCODER_PATH = f\"{BASE_DIR}\\\\label_encoder.joblib\"\n",
    "\n",
    "# Load train/test sets\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Identify feature columns (exclude metadata)\n",
    "exclude_cols = {'pain_level', 'subject', 'file_name', 'audio_path'}\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "X_test = test_df[feature_cols].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['pain_level'].values)\n",
    "y_test = le.transform(test_df['pain_level'].values)\n",
    "\n",
    "# Build pipeline: scaler + XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.03,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Convert predictions back to string labels for reporting\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_labels, y_pred_labels))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Save the trained model and label encoder\n",
    "#joblib.dump(pipeline, MODEL_PATH)\n",
    "#joblib.dump(le, ENCODER_PATH)\n",
    "#print(f\"\\nModel saved as {MODEL_PATH}\")\n",
    "#print(f\"Label encoder saved as {ENCODER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "842d36f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [15:57:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved as: xgb_final_trained_all_features.joblib\n",
      "Label encoder saved as: label_encoder_final.joblib\n",
      "Scaler saved as: scaler_final.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# === Paths ===\n",
    "TRAIN_FEATURES_PATH = \"train_features.csv\"\n",
    "TEST_FEATURES_PATH = \"test_features.csv\"\n",
    "FINAL_MODEL_PATH = \"xgb_final_trained_all_features.joblib\"\n",
    "FINAL_ENCODER_PATH = \"label_encoder_final.joblib\"\n",
    "FINAL_SCALER_PATH = \"scaler_final.joblib\"\n",
    "\n",
    "# === Load train and test data ===\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# === Combine train and test for full data retraining ===\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# === Separate features and labels ===\n",
    "feature_cols = [col for col in full_df.columns if col not in ['pain_level', 'subject', 'file_name', 'audio_path']]\n",
    "X_full = full_df[feature_cols].values\n",
    "y_full = full_df['pain_level']\n",
    "\n",
    "# === Encode labels ===\n",
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(y_full)\n",
    "\n",
    "# === Standardize features ===\n",
    "scaler = StandardScaler()\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "# === Train final model ===\n",
    "final_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "final_model.fit(X_full_scaled, y_full_encoded)\n",
    "\n",
    "# === Save model, encoder, and scaler ===\n",
    "joblib.dump(final_model, FINAL_MODEL_PATH)\n",
    "joblib.dump(le, FINAL_ENCODER_PATH)\n",
    "joblib.dump(scaler, FINAL_SCALER_PATH)\n",
    "\n",
    "print(f\"Final model saved as: {FINAL_MODEL_PATH}\")\n",
    "print(f\"Label encoder saved as: {FINAL_ENCODER_PATH}\")\n",
    "print(f\"Scaler saved as: {FINAL_SCALER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5: Prepare and Unlabel Your Test Set for Prediction\n",
    "\n",
    "- Scans the new test dataset directory with the following structure:\n",
    "    D:\\test\\<pain level>\\<Subject name>\\<audio file>\n",
    "- Extracts subject, file name, and full audio path into a new CSV.\n",
    "- Ignores the pain level column (or sets as 'unknown') for prediction.\n",
    "- Output: test_unlabelled.csv with columns: subject, file_name, audio_path\n",
    "\n",
    "Instructions:\n",
    "- Update TEST_BASE_DIR if your test data is elsewhere.\n",
    "- This file will be used for feature extraction and prediction.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "TEST_BASE_DIR = r'D:\\test'  # Update if necessary\n",
    "rows = []\n",
    "\n",
    "# Loop through the directory structure\n",
    "for pain_level in os.listdir(TEST_BASE_DIR):\n",
    "    pain_dir = os.path.join(TEST_BASE_DIR, pain_level)\n",
    "    if not os.path.isdir(pain_dir):\n",
    "        continue\n",
    "    for subject in os.listdir(pain_dir):\n",
    "        subject_dir = os.path.join(pain_dir, subject)\n",
    "        if not os.path.isdir(subject_dir):\n",
    "            continue\n",
    "        for f in os.listdir(subject_dir):\n",
    "            if f.lower().endswith('.wav'):\n",
    "                rows.append({\n",
    "                    'subject': subject,\n",
    "                    'file_name': f,\n",
    "                    'audio_path': os.path.join(subject_dir, f)\n",
    "                    # Optionally, add 'pain_level': pain_level  # If you want to keep actual for later comparison\n",
    "                })\n",
    "\n",
    "# Save as CSV (no label column for prediction)\n",
    "test_unlabelled_path = os.path.join(TEST_BASE_DIR, 'test_unlabelled.csv')\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(test_unlabelled_path, index=False)\n",
    "print(f\"Created {test_unlabelled_path} with {len(df)} files ready for feature extraction and prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 6: Extract Features from Unlabelled Test Audio\n",
    "\n",
    "- Loads test_unlabelled.csv from Step 5.\n",
    "- For each audio file, extracts the same features as used for training:\n",
    "    - MFCCs (mean, std, min, max, 13 coefficients)\n",
    "    - Delta MFCCs (mean, std, 13 coefficients)\n",
    "    - Chroma STFT (mean, std, 12 coefficients)\n",
    "    - Mel Spectrogram (mean, std, min, max, 128 bands)\n",
    "    - Spectral Contrast (mean, std, 7 bands)\n",
    "    - Tonnetz (mean, std, 6 dimensions)\n",
    "    - Spectral Centroid (mean, std)\n",
    "    - Spectral Bandwidth (mean, std)\n",
    "    - Spectral Rolloff (mean, std)\n",
    "    - Zero Crossing Rate (mean, std)\n",
    "    - RMS energy (mean, std)\n",
    "- Concatenates all features into one row per audio file.\n",
    "- Saves results as test_unlabelled_features.csv in your test data directory.\n",
    "\n",
    "Instructions:\n",
    "- Requires: librosa, numpy, pandas\n",
    "- Update TEST_BASE_DIR if your data is elsewhere.\n",
    "- Run after Step 5.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "TEST_BASE_DIR = r'D:\\test'  # Update if necessary\n",
    "UNLABELLED_CSV = os.path.join(TEST_BASE_DIR, 'test_unlabelled.csv')\n",
    "FEATURES_OUT = os.path.join(TEST_BASE_DIR, 'test_unlabelled_features.csv')\n",
    "SR = 44100\n",
    "N_MFCC = 13\n",
    "N_MELS = 128\n",
    "\n",
    "def extract_features(audio_path):\n",
    "    try:\n",
    "        y, _ = librosa.load(audio_path, sr=SR)\n",
    "        features = []\n",
    "\n",
    "        # MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=N_MFCC)\n",
    "        features.extend(np.mean(mfcc, axis=1))\n",
    "        features.extend(np.std(mfcc, axis=1))\n",
    "        features.extend(np.min(mfcc, axis=1))\n",
    "        features.extend(np.max(mfcc, axis=1))\n",
    "\n",
    "        # Delta MFCCs\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        features.extend(np.mean(mfcc_delta, axis=1))\n",
    "        features.extend(np.std(mfcc_delta, axis=1))\n",
    "\n",
    "        # Chroma STFT\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=SR)\n",
    "        features.extend(np.mean(chroma, axis=1))\n",
    "        features.extend(np.std(chroma, axis=1))\n",
    "\n",
    "        # Mel Spectrogram (log-mel)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=SR, n_mels=N_MELS)\n",
    "        log_mel = librosa.power_to_db(mel)\n",
    "        features.extend(np.mean(log_mel, axis=1))\n",
    "        features.extend(np.std(log_mel, axis=1))\n",
    "        features.extend(np.min(log_mel, axis=1))\n",
    "        features.extend(np.max(log_mel, axis=1))\n",
    "\n",
    "        # Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=SR)\n",
    "        features.extend(np.mean(contrast, axis=1))\n",
    "        features.extend(np.std(contrast, axis=1))\n",
    "\n",
    "        # Tonnetz\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=SR)\n",
    "        features.extend(np.mean(tonnetz, axis=1))\n",
    "        features.extend(np.std(tonnetz, axis=1))\n",
    "\n",
    "        # Spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=SR)\n",
    "        features.append(np.mean(centroid))\n",
    "        features.append(np.std(centroid))\n",
    "\n",
    "        # Spectral bandwidth\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=SR)\n",
    "        features.append(np.mean(bandwidth))\n",
    "        features.append(np.std(bandwidth))\n",
    "\n",
    "        # Spectral rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=SR)\n",
    "        features.append(np.mean(rolloff))\n",
    "        features.append(np.std(rolloff))\n",
    "\n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features.append(np.mean(zcr))\n",
    "        features.append(np.std(zcr))\n",
    "\n",
    "        # RMS energy\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features.append(np.mean(rms))\n",
    "        features.append(np.std(rms))\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        # Return NaNs for all features if error\n",
    "        n_features = (N_MFCC*4 + N_MFCC*2 + 12*2 + N_MELS*4 + 7*2 + 6*2 + 2*5)\n",
    "        return [np.nan] * n_features\n",
    "\n",
    "# Build feature column names\n",
    "feature_names = (\n",
    "    [f\"mfcc_mean_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_std_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_min_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_max_{i+1}\" for i in range(N_MFCC)] +\n",
    "\n",
    "    [f\"mfcc_delta_mean_{i+1}\" for i in range(N_MFCC)] +\n",
    "    [f\"mfcc_delta_std_{i+1}\" for i in range(N_MFCC)] +\n",
    "\n",
    "    [f\"chroma_mean_{i+1}\" for i in range(12)] +\n",
    "    [f\"chroma_std_{i+1}\" for i in range(12)] +\n",
    "\n",
    "    [f\"mel_mean_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_std_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_min_{i+1}\" for i in range(N_MELS)] +\n",
    "    [f\"mel_max_{i+1}\" for i in range(N_MELS)] +\n",
    "\n",
    "    [f\"contrast_mean_{i+1}\" for i in range(7)] +\n",
    "    [f\"contrast_std_{i+1}\" for i in range(7)] +\n",
    "\n",
    "    [f\"tonnetz_mean_{i+1}\" for i in range(6)] +\n",
    "    [f\"tonnetz_std_{i+1}\" for i in range(6)] +\n",
    "\n",
    "    [\"centroid_mean\", \"centroid_std\"] +\n",
    "    [\"bandwidth_mean\", \"bandwidth_std\"] +\n",
    "    [\"rolloff_mean\", \"rolloff_std\"] +\n",
    "    [\"zcr_mean\", \"zcr_std\"] +\n",
    "    [\"rms_mean\", \"rms_std\"]\n",
    ")\n",
    "\n",
    "# Load the unlabelled test metadata\n",
    "df = pd.read_csv(UNLABELLED_CSV)\n",
    "features = [extract_features(row['audio_path']) for _, row in df.iterrows()]\n",
    "feat_df = pd.DataFrame(features, columns=feature_names)\n",
    "result_df = pd.concat([df, feat_df], axis=1)\n",
    "result_df.to_csv(FEATURES_OUT, index=False)\n",
    "print(f\"Advanced feature extraction for test set complete. Saved to {FEATURES_OUT} with shape {result_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 7: Predict Pain Levels on Unlabelled Test Audio Features\n",
    "\n",
    "- Loads extracted features from test_unlabelled_features.csv (from Step 6).\n",
    "- Loads the trained final model, label encoder, and scaler.\n",
    "- Applies **exact same scaling and feature order** as during training.\n",
    "- Predicts pain levels for each unlabelled sample.\n",
    "- Saves results as test_predictions.csv in the test directory, including:\n",
    "    subject, file_name, audio_path, predicted_pain_level\n",
    "\n",
    "Instructions:\n",
    "- Make sure test_unlabelled_features.csv, model, scaler, and label encoder are in the correct paths.\n",
    "- Run after Step 6.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# === Paths ===\n",
    "TEST_BASE_DIR = r'D:\\test'\n",
    "FEATURES_PATH = os.path.join(TEST_BASE_DIR, 'test_unlabelled_features.csv')\n",
    "MODEL_PATH = r'D:\\X-ITE Pain\\xgb_final_trained_all_features.joblib'\n",
    "ENCODER_PATH = r'D:\\X-ITE Pain\\label_encoder_final.joblib'\n",
    "SCALER_PATH = r'D:\\X-ITE Pain\\scaler_final.joblib'\n",
    "OUTPUT_PATH = os.path.join(TEST_BASE_DIR, 'test_predictions.csv')\n",
    "\n",
    "# === Load features ===\n",
    "df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "# === Load model, scaler, and encoder ===\n",
    "model = joblib.load(MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "le = joblib.load(ENCODER_PATH)\n",
    "\n",
    "# === Identify feature columns (exclude metadata) ===\n",
    "exclude_cols = {'subject', 'file_name', 'audio_path'}\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "# === Prepare features ===\n",
    "X = df[feature_cols].values\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# === Predict ===\n",
    "y_pred = model.predict(X_scaled)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# === Save predictions ===\n",
    "results_df = df[['subject', 'file_name', 'audio_path']].copy()\n",
    "results_df['predicted_pain_level'] = y_pred_labels\n",
    "results_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Predictions saved to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
